{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9975797-fbde-4f96-8aac-85a4f950c421",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "\n",
    "The heart disease dataset is a classic dataset that contains various health metrics (age, sex, chest pain type, blood pressure, cholesterol, etc.) related to diagnosing heart disease (binary classification: presence or absence of heart disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cf591d-8a5b-499e-8715-1ad140867934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Loads the heart disease dataset \n",
    "df = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb5ea1c-a4e5-4419-bae8-661fe2d82711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ebc45-d873-4c37-b1e4-ce2b0ebc08f2",
   "metadata": {},
   "source": [
    "We are going to try to predict the presence of hart disease suing this features, starting with a classical baseline method and trying to improve on that result with a series of ensembled approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23ad7e40-87f3-4b93-bef9-a9ddb5881ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separates the data\n",
    "X = df.drop(columns=\"target\")\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Splits data into training and test data sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Scaling of features (for certain models, e.g., SVM or logistic regression, not always necessary for trees)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0153586-1242-43a0-bb61-78b1234434a6",
   "metadata": {},
   "source": [
    "# Baseline model : decision Tree\n",
    "\n",
    "We'll train a decision tree as our baseline model and evaluate it using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d39376f1-b4ca-44c0-8364-d11b9a7605f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.00\n",
      "Testing Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "#Create and Train a Decision Tree Classifier and print the train and test accuracy\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Initializes the Decision Tree Classifier\n",
    "decision_tree = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Trains the Decision Tree model\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Makes predictions\n",
    "y_train_pred = decision_tree.predict(X_train)\n",
    "y_test_pred = decision_tree.predict(X_test)\n",
    "\n",
    "# Evaluates model performance\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Prints the accuracies\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24aab5f2-6ff1-47a0-9039-e648524fcff5",
   "metadata": {},
   "source": [
    "**Training Accuracy:** 1.00\n",
    "**Testing Accuracy:** 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9502aa2-06b0-4cf2-bc6b-cb91390ff1a8",
   "metadata": {},
   "source": [
    "We can see that this model is overfitting. This is expected, decision trees, especially deep ones  are notorious agressive at exploiting the data available. But that also makes them highly variant: a small change on the tree/data makes for potentially large changes in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c60160a-b179-4896-a026-4beab803bb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Training Accuracy = 1.00, Testing Accuracy = 0.69\n",
      "Run 2: Training Accuracy = 1.00, Testing Accuracy = 0.87\n",
      "Run 3: Training Accuracy = 1.00, Testing Accuracy = 0.77\n",
      "Run 4: Training Accuracy = 1.00, Testing Accuracy = 0.82\n",
      "Run 5: Training Accuracy = 1.00, Testing Accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "# Run the same code again a couple of times. \n",
    "# To get different results each time we have to remove the random_state generated the first time around.\n",
    "# Using a loop will make the code more efficient.\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Stores the results for multiple runs\n",
    "results = []\n",
    "\n",
    "# Runs 5 iterations with different random states\n",
    "for seed in range(1, 6):  \n",
    "    # Splits the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Initializes and trains the model\n",
    "    decision_tree = DecisionTreeClassifier(random_state=seed)\n",
    "    decision_tree.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluates the model\n",
    "    train_accuracy = accuracy_score(y_train, decision_tree.predict(X_train))\n",
    "    test_accuracy = accuracy_score(y_test, decision_tree.predict(X_test))\n",
    "    \n",
    "    # Stores results\n",
    "    results.append((seed, train_accuracy, test_accuracy))\n",
    "\n",
    "# Prints the results for all runs\n",
    "for seed, train_acc, test_acc in results:\n",
    "    print(f\"Run {seed}: Training Accuracy = {train_acc:.2f}, Testing Accuracy = {test_acc:.2f}\")\n",
    "\n",
    "# You can see that the Train Accuracy is always 100% (overfitting) and the Test Accuracy is all over the place. \n",
    "# This is undesirable: our method is not generalizing and has high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2d0726-7c1e-47a7-ab08-297a171b6969",
   "metadata": {},
   "source": [
    "**Results from running the loop**\n",
    "Run 1: Training Accuracy = 1.00, Testing Accuracy = 0.69\n",
    "Run 2: Training Accuracy = 1.00, Testing Accuracy = 0.87\n",
    "Run 3: Training Accuracy = 1.00, Testing Accuracy = 0.77\n",
    "Run 4: Training Accuracy = 1.00, Testing Accuracy = 0.82\n",
    "Run 5: Training Accuracy = 1.00, Testing Accuracy = 0.84\n",
    "\n",
    "You can see that the Train Accuracy is always 100% (overfitting) and the Test Accuracy is all over the place. \n",
    "This is undesirable: our method is not generalizing and has high variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb71cb-fc65-4c49-a1d6-1abd9a1085c1",
   "metadata": {},
   "source": [
    "# Bagging: reducing variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f3606-bb0b-4567-8583-11c38ab02579",
   "metadata": {},
   "source": [
    "Bagging improves models because it reduces variance by averaging the predictions of multiple models trained on different subsets of the training data. This averaging effect reduces the sensitivity of the overall model to any one dataset or model, making the final prediction more stable and less prone to overfitting.\n",
    "\n",
    "- High-variance models, like decision trees, tend to overfit the training data. This means that small changes in the training data can lead to large changes in the model’s predictions. For example, a decision tree trained on one subset of data might look completely different from a decision tree trained on another subset. This leads to high variance, where the model’s performance fluctuates a lot depending on the specific data it was trained on.\n",
    "- Once all the individual models are trained, Bagging combines their predictions by averaging them (for regression) or using a majority vote (for classification). The key idea here is that the errors in each individual model are somewhat independent because they are trained on different bootstrap samples. Some models will make errors in one direction, while others might make errors in another. When you average these predictions, the errors cancel out, reducing the overall variability (variance) of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8fc76766-a90c-47ed-bd02-66827a1dc115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.83\n",
      "Testing Accuracy: 0.90\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a BaggingClassifier. \n",
    "# Use as base estimator a weak decision tree (max_depth=1) and 100 estimators to really over a lot of different data samples\n",
    "# Print the train and test accuracy\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Splits the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize a weak decision tree with max_depth=1\n",
    "weak_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Creates the bagging classifier with 100 estimators\n",
    "bagging_clf = BaggingClassifier(estimator=weak_tree, n_estimators=100, random_state=42)\n",
    "\n",
    "# Trains the bagging classifier\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "\n",
    "# Makes predictions\n",
    "y_train_pred = bagging_clf.predict(X_train)\n",
    "y_test_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "# Evaluates performance\n",
    "# Calculates the training and test accuracy scores\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Prints the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efabafd-e27b-4a70-85e2-159170853f0b",
   "metadata": {},
   "source": [
    "You can probably see a modest improvement in score, but most importantly, the overfitting is mostly gone. This is because averaging over multiple datasets stabilizes the high variance of the base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9f892484-618a-46fe-8e56-0a18fa652ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Training Accuracy = 0.78, Testing Accuracy = 0.67\n",
      "Run 2: Training Accuracy = 0.84, Testing Accuracy = 0.85\n",
      "Run 3: Training Accuracy = 0.76, Testing Accuracy = 0.77\n",
      "Run 4: Training Accuracy = 0.75, Testing Accuracy = 0.80\n",
      "Run 5: Training Accuracy = 0.82, Testing Accuracy = 0.92\n"
     ]
    }
   ],
   "source": [
    "# Runs the same code a couple of times, using a loop.\n",
    "\n",
    "# Stores the results for multiple runs\n",
    "results_mult_bagging = []\n",
    "\n",
    "# Runs 5 iterations with different random states\n",
    "for seed in range(1, 6):  \n",
    "    # Splits the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Initializes the model \n",
    "    # Initialize a weak decision tree with max_depth=1\n",
    "    weak_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "    # Creates the bagging classifier with 100 estimators\n",
    "    bagging_clf = BaggingClassifier(estimator=weak_tree, n_estimators=100, random_state=42)\n",
    "\n",
    "    # Trains the bagging classifier\n",
    "    bagging_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Makes predictions\n",
    "    y_train_pred = bagging_clf.predict(X_train)\n",
    "    y_test_pred = bagging_clf.predict(X_test)\n",
    "\n",
    "    # Calculates accuracy for training and testing sets\n",
    "    train_acc = accuracy_score(y_train, y_train_pred)\n",
    "    test_acc = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    # Appends results to the list\n",
    "    results_mult_bagging.append((seed, train_acc, test_acc))\n",
    "\n",
    "# Prints the results for all runs\n",
    "for seed, train_acc, test_acc in results_mult_bagging:\n",
    "    print(f\"Run {seed}: Training Accuracy = {train_acc:.2f}, Testing Accuracy = {test_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62813807-2bae-4ea5-a38f-b76df6b26879",
   "metadata": {},
   "source": [
    "**Results**\n",
    "\n",
    "Run 1: Training Accuracy = 0.78, Testing Accuracy = 0.67\n",
    "\n",
    "Run 2: Training Accuracy = 0.84, Testing Accuracy = 0.85\n",
    "\n",
    "Run 3: Training Accuracy = 0.76, Testing Accuracy = 0.77\n",
    "\n",
    "Run 4: Training Accuracy = 0.75, Testing Accuracy = 0.80\n",
    "\n",
    "Run 5: Training Accuracy = 0.82, Testing Accuracy = 0.92\n",
    "\n",
    "You can see that consistently the Train Accuracy is close to the Test Accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99849f-20fe-4eac-be80-b43dd56ba374",
   "metadata": {},
   "source": [
    "# Boosting: reducing bias\n",
    "\n",
    "Now we’ll apply AdaBoost with decision trees as weak learners. This will sequentially improve the model by focusing on difficult cases.\n",
    "\n",
    "Boosting reduces bias by sequentially training a series of weak learners (often simple models like decision trees) where each subsequent model focuses on the mistakes made by the previous models. The key idea behind boosting is to incrementally improve the model by correcting errors, which helps to reduce bias, especially when the initial model is too simple and underfits the data.\n",
    "\n",
    "- Boosting typically uses weak learners, which are models that perform only slightly better than random guessing. For example, in classification, a weak learner might be a shallow decision tree (a \"stump\") with just a few levels. Weak learners usually have high bias, meaning they are too simplistic and don't capture the underlying patterns in the data well. As a result, they underfit the data.\n",
    "\n",
    "- In each iteration, boosting trains a new model that tries to correct the errors made by the earlier models. If an instance was misclassified by the first weak learner, it will receive a higher weight, so the next model pays more attention to it. As the sequence of models progresses, the ensemble collectively focuses more on the difficult-to-predict instances. Over time, the combined models become better at fitting the data, as they successively reduce the bias (systematic error) by adjusting for earlier mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4bba1773-b0b0-44ba-a838-58b8c466ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.90\n",
      "Testing Accuracy: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Creates and trains an AdaBoostClassifier. \n",
    "# Uses as base estimator a weak decision tree (max_depth=1) and 100 estimators to really target the specific behaviors of this phenomenon\n",
    "# Prints the train and test accuracies\n",
    "\n",
    "# Imports the necessary libraries\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Splits the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initializes a weak decision tree with max_depth=1\n",
    "weak_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "\n",
    "# Creates and trains the AdaBoost classifier\n",
    "# ORIGINAL code - removed due to future warning received that the default is deprecated.\n",
    "    # adaboost_clf = AdaBoostClassifier(estimator=weak_tree, n_estimators=100, random_state=42)\n",
    "adaboost_clf = AdaBoostClassifier(estimator=weak_tree, n_estimators=100, algorithm='SAMME', random_state=42)\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# Makes predictions\n",
    "y_train_pred = adaboost_clf.predict(X_train)\n",
    "y_test_pred = adaboost_clf.predict(X_test)\n",
    "\n",
    "# Evaluates train and test performance\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Prints the accuracies\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427e8a3-478b-4260-b2ac-74aa80983e50",
   "metadata": {},
   "source": [
    "You can probably see a good improvement in score, but overfitting rearing it's ugly head a gain (not as much as in the base model). This is because the iterative correction of adaboost really allows the model to focus on the specifics of this problem, at a cost of overexploiting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b5e21fe-0a8f-45f6-a2d3-74261941f9c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: Training Accuracy = 0.92, Testing Accuracy = 0.74\n",
      "Run 2: Training Accuracy = 0.89, Testing Accuracy = 0.89\n",
      "Run 3: Training Accuracy = 0.89, Testing Accuracy = 0.89\n",
      "Run 4: Training Accuracy = 0.90, Testing Accuracy = 0.85\n",
      "Run 5: Training Accuracy = 0.90, Testing Accuracy = 0.84\n"
     ]
    }
   ],
   "source": [
    "# Runs the same code again a couple of times. \n",
    "\n",
    "# Stores the results for multiple runs\n",
    "results_mult_boosting = []\n",
    "\n",
    "# Runs 5 iterations with different random states\n",
    "for seed in range(1, 6):  \n",
    "    # Splits the dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=seed)\n",
    "    \n",
    "    # Initializes the model \n",
    "    # Initialize a weak decision tree with max_depth=1\n",
    "    weak_tree = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
    "    \n",
    "    # Creates the ADABoost classifier with 100 estimators\n",
    "    adaboost_clf = AdaBoostClassifier(estimator=weak_tree, n_estimators=100, algorithm='SAMME', random_state=42)\n",
    "\n",
    "    # Trains the bagging classifier\n",
    "    adaboost_clf.fit(X_train, y_train)\n",
    "    \n",
    "    # Makes predictions\n",
    "    y_train_pred = adaboost_clf.predict(X_train)\n",
    "    y_test_pred = adaboost_clf.predict(X_test)\n",
    "\n",
    "    # Evaluates train and test performance\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "     # Appends the results to the list\n",
    "    results_mult_boosting.append((seed, train_accuracy, test_accuracy))\n",
    "\n",
    "# Prints the results for all runs\n",
    "for seed, train_acc, test_acc in results_mult_boosting:\n",
    "    print(f\"Run {seed}: Training Accuracy = {train_acc:.2f}, Testing Accuracy = {test_acc:.2f}\")\n",
    "\n",
    "# You can see that the test Accuracy will mostly be pretty good, even if some times it get's lower or higher scores (high variance, low bias)\n",
    "# You can see also that consistently the Train Accuracy is higher than the Test Accuracy,indicating some (not extreme) overfitting \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1d8531-8ab4-4965-9996-8bd218c1c32e",
   "metadata": {},
   "source": [
    "**Results**\n",
    "Run 1: Training Accuracy = 0.92, Testing Accuracy = 0.74\n",
    "Run 2: Training Accuracy = 0.89, Testing Accuracy = 0.89\n",
    "Run 3: Training Accuracy = 0.89, Testing Accuracy = 0.89\n",
    "Run 4: Training Accuracy = 0.90, Testing Accuracy = 0.85\n",
    "Run 5: Training Accuracy = 0.90, Testing Accuracy = 0.84\n",
    "\n",
    "You can see that the test Accuracy will mostly be pretty good, even if some times it get's lower or higher scores (high variance, low bias)\n",
    "You can see also that consistently the Train Accuracy is higher than the Test Accuracy,indicating some (not extreme) overfitting \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
