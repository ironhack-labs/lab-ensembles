{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9975797-fbde-4f96-8aac-85a4f950c421",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "\n",
    "The heart disease dataset is a classic dataset that contains various health metrics (age, sex, chest pain type, blood pressure, cholesterol, etc.) related to diagnosing heart disease (binary classification: presence or absence of heart disease)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cf591d-8a5b-499e-8715-1ad140867934",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset (change the path if needed)\n",
    "df = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb5ea1c-a4e5-4419-bae8-661fe2d82711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trestbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalach</th>\n",
       "      <th>exang</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slope</th>\n",
       "      <th>ca</th>\n",
       "      <th>thal</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>145</td>\n",
       "      <td>233</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>130</td>\n",
       "      <td>250</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>130</td>\n",
       "      <td>204</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>172</td>\n",
       "      <td>0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>120</td>\n",
       "      <td>236</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>120</td>\n",
       "      <td>354</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>163</td>\n",
       "      <td>1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age  sex  cp  trestbps  chol  fbs  restecg  thalach  exang  oldpeak  slope  \\\n",
       "0   63    1   3       145   233    1        0      150      0      2.3      0   \n",
       "1   37    1   2       130   250    0        1      187      0      3.5      0   \n",
       "2   41    0   1       130   204    0        0      172      0      1.4      2   \n",
       "3   56    1   1       120   236    0        1      178      0      0.8      2   \n",
       "4   57    0   0       120   354    0        1      163      1      0.6      2   \n",
       "\n",
       "   ca  thal  target  \n",
       "0   0     1       1  \n",
       "1   0     2       1  \n",
       "2   0     2       1  \n",
       "3   0     2       1  \n",
       "4   0     2       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ebc45-d873-4c37-b1e4-ce2b0ebc08f2",
   "metadata": {},
   "source": [
    "We are going to try to predict the presence of heart disease suing this features, starting with a classical baseline method and trying to improve on that result with a series of ensembled approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "23ad7e40-87f3-4b93-bef9-a9ddb5881ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=\"target\")\n",
    "y = df[\"target\"]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Feature scaling (for certain models, e.g., SVM or logistic regression, not always necessary for trees)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0153586-1242-43a0-bb61-78b1234434a6",
   "metadata": {},
   "source": [
    "# Baseline model : decision Tree\n",
    "\n",
    "We'll train a decision tree as our baseline model and evaluate it using accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d39376f1-b4ca-44c0-8364-d11b9a7605f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Accuracy: 1.00\n",
      "Testing Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "#Create and Train a Decision Tree Classifier and print the train and test accuracy\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "# Create and Train a Decision Tree Classifier\n",
    "# Initialize the Decision Tree Classifier\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "# Make predictions on both the training and testing datasets\n",
    "y_train_pred = classifier.predict(X_train_scaled)\n",
    "y_test_pred = classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "# Compute accuracy for both training and testing datasets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\nTraining Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28cccc8-b6a4-4fa5-9eff-4aca34a26d03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export Tree\n",
    "import os\n",
    "from sklearn import tree\n",
    "\n",
    "dotfile = tree.export_graphviz(classifier, out_file = \"heart_tree.dot\")\n",
    "\n",
    "os.system(\"dot -Tpng -Gdpi=48 heart_tree.dot -o heart_tree.png\") # Change the -Gdpi=48 to increase the Image Size or Resolution\n",
    "\n",
    "from IPython.core.display import Image\n",
    "\n",
    "# Get the number of nodes and depth of the tree\n",
    "num_nodes = classifier.tree_.node_count\n",
    "tree_depth = classifier.tree_.max_depth\n",
    "# Print the number of nodes and depth\n",
    "print(f\"Number of Nodes in the Tree: {num_nodes}\")\n",
    "print(f\"Depth of the Tree: {tree_depth}\")\n",
    "print()\n",
    "\n",
    "# Mostrar el PNG\n",
    "Image(\"heart_tree.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9502aa2-06b0-4cf2-bc6b-cb91390ff1a8",
   "metadata": {},
   "source": [
    "We can see that this model is overfitting. This is expected, decision trees, especially deep ones  are notorious agressive at exploiting the data available. But that also makes them highly variant: a small change on the tree/data makes for potentially large changes in performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c60160a-b179-4896-a026-4beab803bb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same code again a couple of times. \n",
    "# You can see that the Train Accuracy is always 100% (overfitting) and the Test Accuracy is all over the place. \n",
    "# This is undesirable: our method is not generalizing and has high variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9b824e9e-ffbb-4656-9290-b22859a2b02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 1.00\n",
      "Testing Accuracy: 0.79\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a Decision Tree Classifier\n",
    "# Initialize the Decision Tree Classifier\n",
    "classifier = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Train the classifier on the training set\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "# Make predictions on both the training and testing datasets\n",
    "y_train_pred = classifier.predict(X_train_scaled)\n",
    "y_test_pred = classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "# Compute accuracy for both training and testing datasets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfb71cb-fc65-4c49-a1d6-1abd9a1085c1",
   "metadata": {},
   "source": [
    "# Bagging: reducing variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828f3606-bb0b-4567-8583-11c38ab02579",
   "metadata": {},
   "source": [
    "Bagging improves models because it reduces variance by averaging the predictions of multiple models trained on different subsets of the training data. This averaging effect reduces the sensitivity of the overall model to any one dataset or model, making the final prediction more stable and less prone to overfitting.\n",
    "\n",
    "- High-variance models, like decision trees, tend to overfit the training data. This means that small changes in the training data can lead to large changes in the model’s predictions. For example, a decision tree trained on one subset of data might look completely different from a decision tree trained on another subset. This leads to high variance, where the model’s performance fluctuates a lot depending on the specific data it was trained on.\n",
    "- Once all the individual models are trained, Bagging combines their predictions by averaging them (for regression) or using a majority vote (for classification). The key idea here is that the errors in each individual model are somewhat independent because they are trained on different bootstrap samples. Some models will make errors in one direction, while others might make errors in another. When you average these predictions, the errors cancel out, reducing the overall variability (variance) of the final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8fc76766-a90c-47ed-bd02-66827a1dc115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 0.76\n",
      "Testing Accuracy: 0.76\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a BaggingClassifier. \n",
    "# Use as base estimator a weak decision tree (max_depth=1) and 100 estimators to really over a lot of different data samples\n",
    "# Print the train and test accuracy\n",
    "\n",
    "seed = 42\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Create and Train a BaggingClassifier\n",
    "# Initialize the BaggingClassifier with a weak decision tree as base estimator\n",
    "bagging_classifier = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1, random_state=seed), # it's best with max_depth=5\n",
    "    n_estimators=10,\n",
    "    random_state=seed\n",
    ")\n",
    "\n",
    "# Train the BaggingClassifier on the training set\n",
    "bagging_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predictions and evaluation\n",
    "# Make predictions on both the training and testing datasets\n",
    "y_train_pred = bagging_classifier.predict(X_train_scaled)\n",
    "y_test_pred = bagging_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "# Compute accuracy for both training and testing datasets\n",
    "train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Training Accuracy: {train_accuracy:.2f}\")\n",
    "print(f\"Testing Accuracy: {test_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7392f6fe-af2b-4f8b-b9bc-f37746331c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Access the first decision tree in the ensemble\n",
    "first_tree = bagging_classifier.estimators_[0]\n",
    "\n",
    "# Export the tree to a DOT file\n",
    "export_graphviz(\n",
    "    first_tree,\n",
    "    out_file=\"bagging_first_tree.dot\",\n",
    "    feature_names=[f'Feature {i}' for i in range(X_train_scaled.shape[1])],  # Placeholder names\n",
    "    class_names=['Class 0', 'Class 1'],  # Replace with your actual class names if available\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True\n",
    ")\n",
    "\n",
    "# Convert the DOT file to a PNG\n",
    "os.system(\"dot -Tpng -Gdpi=45 bagging_first_tree.dot -o bagging_first_tree.png\") # Change the -Gdpi=45 to increase the Image Size or Resolution\n",
    "\n",
    "# Display the image\n",
    "Image(\"bagging_first_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f3c9f659-8287-4e58-9658-fc6373b1e41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Export all trees in the bagging classifier\n",
    "\n",
    "# for i, tree in enumerate(bagging_classifier.estimators_):\n",
    "#     export_graphviz(\n",
    "#         tree,\n",
    "#         out_file=f\"tree_{i}.dot\",\n",
    "#         feature_names=[f'Feature {i}' for i in range(X_train_scaled.shape[1])],  # Replace with actual feature names\n",
    "#         class_names=['Class 0', 'Class 1'],  # Replace with actual class names\n",
    "#         filled=True,\n",
    "#         rounded=True,\n",
    "#         special_characters=True\n",
    "#     )\n",
    "#     os.system(f\"dot -Tpng tree_{i}.dot -o tree_{i}.png\")\n",
    "#     print(f\"Tree {i} saved as tree_{i}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efabafd-e27b-4a70-85e2-159170853f0b",
   "metadata": {},
   "source": [
    "You can probably see a modest improvement in score, but most importantly, the overfitting is mostly gone. This is because averaging over multiple datasets stabilizes the high variance of the base model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f892484-618a-46fe-8e56-0a18fa652ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same code again a couple of times. \n",
    "# You can see that consistently the Train Accuracy is close to the Test Accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99849f-20fe-4eac-be80-b43dd56ba374",
   "metadata": {},
   "source": [
    "# Boosting: reducing bias\n",
    "\n",
    "Now we’ll apply AdaBoost with decision trees as weak learners. This will sequentially improve the model by focusing on difficult cases.\n",
    "\n",
    "Boosting reduces bias by sequentially training a series of weak learners (often simple models like decision trees) where each subsequent model focuses on the mistakes made by the previous models. The key idea behind boosting is to incrementally improve the model by correcting errors, which helps to reduce bias, especially when the initial model is too simple and underfits the data.\n",
    "\n",
    "- Boosting typically uses weak learners, which are models that perform only slightly better than random guessing. For example, in classification, a weak learner might be a shallow decision tree (a \"stump\") with just a few levels. Weak learners usually have high bias, meaning they are too simplistic and don't capture the underlying patterns in the data well. As a result, they underfit the data.\n",
    "\n",
    "- In each iteration, boosting trains a new model that tries to correct the errors made by the earlier models. If an instance was misclassified by the first weak learner, it will receive a higher weight, so the next model pays more attention to it. As the sequence of models progresses, the ensemble collectively focuses more on the difficult-to-predict instances. Over time, the combined models become better at fitting the data, as they successively reduce the bias (systematic error) by adjusting for earlier mistakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "4bba1773-b0b0-44ba-a838-58b8c466ff88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AdaBoost - Training Accuracy: 0.91\n",
      "AdaBoost - Testing Accuracy: 0.87\n"
     ]
    }
   ],
   "source": [
    "# Create and Train a AdaBoostClassifier. \n",
    "# Use as base estimator a weak decision tree (max_depth=1) and 100 estimators to really target the specific behaviors of this phenomenon\n",
    "# Print the train and test accuracy\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Create and Train an AdaBoostClassifier\n",
    "# Initialize the AdaBoostClassifier with a weak decision tree as base estimator\n",
    "adaboost_classifier = AdaBoostClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=1),\n",
    "    n_estimators=100,\n",
    "    algorithm='SAMME',  # Explicitly specify the algorithm\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "# Train the AdaBoostClassifier on the training set\n",
    "adaboost_classifier.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Task 5: Predictions and evaluation for AdaBoost\n",
    "# Make predictions on both the training and testing datasets\n",
    "y_train_pred_adaboost = adaboost_classifier.predict(X_train_scaled)\n",
    "y_test_pred_adaboost = adaboost_classifier.predict(X_test_scaled)\n",
    "\n",
    "# Task 6: Evaluate performance for AdaBoost\n",
    "# Compute accuracy for both training and testing datasets\n",
    "train_accuracy_adaboost = accuracy_score(y_train, y_train_pred_adaboost)\n",
    "test_accuracy_adaboost = accuracy_score(y_test, y_test_pred_adaboost)\n",
    "\n",
    "# Print the results for AdaBoost\n",
    "print(f\"AdaBoost - Training Accuracy: {train_accuracy_adaboost:.2f}\")\n",
    "print(f\"AdaBoost - Testing Accuracy: {test_accuracy_adaboost:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4427e8a3-478b-4260-b2ac-74aa80983e50",
   "metadata": {},
   "source": [
    "You can probably see a good improvement in score, but overfitting rearing it's ugly head a gain (not as much as in the base model). This is because the iterative correction of adaboost really allows the model to focus on the specifics of this problem, at a cost of overexploiting the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67684394-32d5-48c8-88e5-d2e20efed142",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import export_graphviz\n",
    "from IPython.display import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Access the first decision tree in the ensemble\n",
    "first_tree = adaboost_classifier.estimators_[0]\n",
    "\n",
    "# Export the tree to a DOT file\n",
    "export_graphviz(\n",
    "    first_tree,\n",
    "    out_file=\"adaboost_first_tree.dot\",\n",
    "    feature_names=[f'Feature {i}' for i in range(X_train_scaled.shape[1])],  # Placeholder names\n",
    "    class_names=['Class 0', 'Class 1'],  # Replace with your actual class names if available\n",
    "    filled=True,\n",
    "    rounded=True,\n",
    "    special_characters=True\n",
    ")\n",
    "\n",
    "# Convert the DOT file to a PNG\n",
    "os.system(\"dot -Tpng -Gdpi=45 adaboost_first_tree.dot -o adaboost_first_tree.png\")  # Change the -Gdpi=45 to increase the Image Size or Resolution\n",
    "\n",
    "# Display the image\n",
    "Image(\"adaboost_first_tree.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5e21fe-0a8f-45f6-a2d3-74261941f9c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the same code again a couple of times. \n",
    "# You can see that the test Accuracy will mostly be pretty good, even if some times it get's lower or higher scores (high variance, low bias)\n",
    "# You can see also that consistently the Train Accuracy is higher than the Test Accuracy,indicating some (not extreme) overfitting "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
